from __future__ import annotations

import itertools
import math
import os
import time  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ç”¨
from typing import Dict, FrozenSet, List

from dotenv import load_dotenv
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI

from shapley_tools import tools
from shapley_decomposed_agent import PaperWorkflowAgent


load_dotenv()

# ------------------------------------------------------------------
# 1. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ãƒ¢ãƒ‡ãƒ«ã®å®šç¾© (Base: Llama, Target: GPT)
# ------------------------------------------------------------------
def build_agent_models() -> Dict[str, object]:
    """
    baseï¼šLlamaï¼ˆOllamaï¼‰
    targetï¼šGPTï¼ˆOpenAIï¼‰
    """
    base_model_name = os.environ.get("OLLAMA_MODEL", "llama3.1")
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ gpt-4o æ¨å¥¨ï¼‰
    target_model_name = os.environ.get("OPENAI_MODEL", "gpt-4o")

    return {
        "base": ChatOllama(model=base_model_name),
        "target": ChatOpenAI(
            model=target_model_name,
            temperature=0,
            request_timeout=120,
        ),
    }

agent_models = build_agent_models()

# ------------------------------------------------------------------
# 2. åˆ¤å®šç”¨ãƒ¢ãƒ‡ãƒ«ã®å®šç¾© (Judge: GPT-4o)
# ------------------------------------------------------------------
# åˆ¤å®šã«ã¯å¸¸ã«é«˜æ€§èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™
judge_llm = ChatOpenAI(
    model="gpt-4o", 
    temperature=0,
    api_key=os.environ.get("OPENAI_API_KEY")
)

# ------------------------------------------------------------------
# 3. è©•ä¾¡ã‚¿ã‚¹ã‚¯
# ------------------------------------------------------------------
evaluation_tasks = [
    {"query": "ã“ã‚Œã‹ã‚‰ä½œæ¥­ã‚’ã™ã‚‹å ´æ‰€ã‚’æ±ºã‚ãŸã„ã®ã§ã€è‡ªå®…ã¨ç ”ç©¶å®¤ã®ç’°å¢ƒï¼ˆCO2æ¿ƒåº¦ã‚„æ¸©åº¦ï¼‰ã‚’æ¯”è¼ƒã—ã¦ã€ã‚ˆã‚Šå¿«é©ãªæ–¹ã‚’æ•™ãˆã¦ã€‚"},
    {"query": "ä»Šã®ç§ã®å¿ƒæ‹æ•°ãŒå¹³å¸¸æ™‚ã‚ˆã‚Šé«˜ã„ã‚ˆã†ãªã‚‰ã€ãƒªãƒ©ãƒƒã‚¯ã‚¹ã§ãã‚‹ã‚ˆã†ã«ã‚¨ã‚¢ã‚³ãƒ³ã‚’å†·æˆ¿ã«ã—ã¦å®¤æ¸©ã‚’å°‘ã—ä¸‹ã’ã¦ã€‚"},
    {"query": "æ¬¡ã®ä¼šè­°ã®é–‹å§‹æ™‚é–“ã‚’ç¢ºèªã—ã¦ã€ãã®æ™‚åˆ»ã¾ã§ã«éƒ¨å±‹ãŒå¿«é©ãªæ¸©åº¦ã«ãªã‚‹ã‚ˆã†ã«ã€ä»Šã‹ã‚‰ã‚¨ã‚¢ã‚³ãƒ³ã‚’èª¿æ•´ã—ã¦ãŠã„ã¦ã€‚"},
    {"query": "ã‚‚ã—æ¹¿åº¦ãŒ40%ä»¥ä¸‹ã§ã€ã‹ã¤æ°—æ¸©ãŒ20åº¦ã‚’ä¸‹å›ã£ã¦ã„ã‚‹ãªã‚‰ã€é¢¨é‚ªäºˆé˜²ã®ãŸã‚ã«åŠ æ¹¿å™¨ã¨æš–æˆ¿ã‚’ä¸¡æ–¹ã¨ã‚‚ONã«ã—ã¦ã€‚"},
    {"query": "å¤–ã®æ°—æ¸©ã¨å®¤å†…ã®æ°—æ¸©ã‚’ç¢ºèªã—ã¦ã€ã‚‚ã—å®¤å†…ã®æ–¹ãŒæš‘ã‘ã‚Œã°ã‚¨ã‚¢ã‚³ãƒ³ã‚’å†·æˆ¿ã§ã¤ã‘ã¦ã€é€†ãªã‚‰çª“ã‚’é–‹ã‘ã‚‹ã‚ˆã†ï¼ˆæ›æ°—ï¼‰ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã—ã¦ã€‚"},
    {"query": "ç ”ç©¶å®¤ã®å¿ƒæ‹ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ã€èª°ã‚‚ã„ãªã•ãã†ãªã‚‰ä»Šæ—¥ã¯è‡ªå®…ã§ä»•äº‹ã‚’ã™ã‚‹ã®ã§ã€è‡ªå®…ã®ç’°å¢ƒã‚’æ•´ãˆã¦ã€‚"},
    {"query": "æ˜æ—¥ãŒé›¨äºˆå ±ãªã‚‰æ¹¿åº¦ãŒä¸ŠãŒã‚‹ã¯ãšãªã®ã§ã€ä»Šã®ã†ã¡ã«åŠ æ¹¿å™¨ã‚’OFFã«ã—ã¦ã€ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã«ã€Œå‚˜ã‚’æŒã£ã¦ã„ãã€ã¨è¿½åŠ ã—ã¦ã€‚"},
    {"query": "CO2æ¿ƒåº¦ãŒ1000ppmã‚’è¶…ãˆã¦ã„ã‚‹ãªã‚‰é›†ä¸­åŠ›ãŒä¸‹ãŒã‚‹ã®ã§æ•™ãˆã¦ã€‚ã‚‚ã—è¶…ãˆã¦ã„ãªã‘ã‚Œã°ã€ãã®ã¾ã¾ã‚¨ã‚¢ã‚³ãƒ³ã§æ¸©åº¦ã ã‘ç¶­æŒã—ã¦ã€‚"},
    {"query": "ã‚‚ã†ã™ãå¯ã‚‹ã®ã§ã€éƒ¨å±‹ãŒä¹¾ç‡¥ã—ã™ãã¦ã„ãªã„ã‹ç¢ºèªã—ã¦ã€‚å•é¡Œãªã‘ã‚Œã°åŠ æ¹¿å™¨ã¯æ“ä½œã›ãšã€ã‚¨ã‚¢ã‚³ãƒ³ã ã‘OFFã«ã—ã¦ã€‚"},
    {"query": "åˆå‰ä¸­ã®å¤©æ°—ãŒè’ã‚Œãã†ãªã‚‰ã€ã‚¨ã‚¢ã‚³ãƒ³ã‚’ã¤ã‘ã¦æš–ã‚ã¦ã€‚"},
    {"query": "ä»Šã€éƒ¨å±‹ï¼ˆè‡ªå®…ï¼‰ã®CO2æ¿ƒåº¦ãŒä½ãã€ã‹ã¤ã‚¨ã‚¢ã‚³ãƒ³ãŒã¤ã„ã¦ã„ã‚‹ã‚ˆã†ãªã‚‰ã€ç„¡é§„ãªã®ã§OFFã«ã—ã¦ãŠã„ã¦ã€‚"},
    {"query": "æœ€è¿‘ä½“èª¿ãŒå„ªã‚Œãªã„ã®ã§ã€ä»Šã®éƒ¨å±‹ã®ç’°å¢ƒï¼ˆæ¸©æ¹¿åº¦ãƒ»CO2ï¼‰ã¨ç§ã®å¿ƒæ‹æ•°ã‚’è¦‹ã¦ã€å¥åº·ã«æ‚ªãã†ãªè¦å› ãŒã‚ã‚Œã°è§£æ¶ˆã—ã¦ã€‚"},
    {"query": "ä»Šã®å¤©æ°—ã€å®¤æ¸©ã€æ¹¿åº¦ã€CO2ã‚’ç·åˆçš„ã«åˆ¤æ–­ã—ã¦ã€ç§ãŒä»Šä¸€ç•ªå¿«é©ã«éã”ã›ã‚‹è¨­å®šã«ã‚¨ã‚¢ã‚³ãƒ³ã¨åŠ æ¹¿å™¨ã‚’è‡ªå‹•ã§ã‚»ãƒƒãƒˆã—ã¦ã€‚"},
    {"query": "ä»Šæ—¥ã®åˆå¾Œã«äºˆå®šãŒå…¥ã£ã¦ã„ãªã‘ã‚Œã°ã€15æ™‚ã‹ã‚‰1æ™‚é–“ã€Œé›†ä¸­ä½œæ¥­ã€ã¨ã„ã†äºˆå®šã‚’å…¥ã‚Œã¦ã€‚"},
    {"query": "å¸°å®…ã—ãŸã°ã‹ã‚Šã§éƒ¨å±‹ãŒã™ã”ãæš‘ã„æ°—ãŒã™ã‚‹ã€‚ä»Šã®æ¸©åº¦ã‚’ç¢ºèªã—ã¦ã€28åº¦ä»¥ä¸Šãªã‚‰æ€¥é€Ÿå†·æˆ¿ã§ã™ãã«æ¶¼ã—ãã—ã¦ã€‚"}
]


# ------------------------------------------------------------------
# 4. GPTã«ã‚ˆã‚‹æˆåŠŸåˆ¤å®šé–¢æ•°ï¼ˆã“ã“ã‚’ä¿®æ­£ã—ã¾ã—ãŸï¼‰
# ------------------------------------------------------------------
def evaluate_success(response: str, task: dict) -> bool:
    if not response:
        return False

    query = task["query"]
    
    # åˆ¤å®šç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼šè©•ä¾¡åŸºæº–ã‚’æ˜ç¢ºã«æŒ‡ç¤º
    prompt = f"""
    ã‚ãªãŸã¯IoTã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å‹•ä½œè©•ä¾¡è€…ã§ã™ã€‚
    ä»¥ä¸‹ã®ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã€ã«å¯¾ã—ã¦ã€ã€Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å›ç­”ã€ãŒé©åˆ‡ã‹ã©ã†ã‹ã‚’å³æ ¼ã«åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

    ### ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚
    {query}

    ### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å›ç­”
    {response}

    ### åˆ¤å®šåŸºæº–
    1. è¦æ±‚ã•ã‚ŒãŸæƒ…å ±ï¼ˆæ•°å€¤ã‚„çŠ¶æ…‹ï¼‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿï¼ˆä¾‹ï¼šã€Œæ¸©åº¦ã‚’æ•™ãˆã¦ã€ã«å¯¾ã—ã€Œ25åº¦ã§ã™ã€ã¨ç­”ãˆã¦ã„ã‚‹ã‹ï¼‰
    2. æ¡ä»¶ä»˜ãã®æŒ‡ç¤ºï¼ˆã‚‚ã—ã€‡ã€‡ãªã‚‰Ã—Ã—ã—ã¦ï¼‰ã«å¯¾ã—ã€æ¡ä»¶åˆ¤å®šã‚’è¡Œã£ãŸå½¢è·¡ãŒã‚ã‚‹ã‹ï¼Ÿ
    3. å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ã‚„ã€Œã§ãã¾ã›ã‚“ã§ã—ãŸã€ã¨ã„ã†å†…å®¹ã§çµ‚ã‚ã£ã¦ã„ãªã„ã‹ï¼Ÿ
    4. æœ€çµ‚çš„ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç›®çš„ãŒé”æˆã•ã‚ŒãŸã‹ï¼Ÿ

    ### å‡ºåŠ›å½¢å¼
    æˆåŠŸã®å ´åˆã¯ "SUCCESS" ã€å¤±æ•—ã®å ´åˆã¯ "FAILURE" ã¨ã ã‘å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ä½™è¨ˆãªæ–‡ç« ã¯ä¸è¦ã§ã™ã€‚
    """

    try:
        # GPT-4oã«åˆ¤å®šã•ã›ã‚‹
        judgment = judge_llm.invoke(prompt).content.strip()
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨ã«åˆ¤å®šçµæœã‚’è¡¨ç¤ºã—ã¦ã‚‚è‰¯ã„
        # print(f"    [Judge Result]: {judgment}")

        return "SUCCESS" in judgment
        
    except Exception as e:
        print(f"    [Judge Error] {e}")
        return False


# ------------------------------------------------------------------
# 5. è©•ä¾¡å®Ÿè¡Œãƒ«ãƒ¼ãƒ—
# ------------------------------------------------------------------
def run_evaluation() -> Dict[FrozenSet[str], float]:
    print("ğŸ¤– è«–æ–‡ã«åŸºã¥ã„ãŸ4ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ä½“ç³»çš„è©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™ï¼")
    print("   Target Model: OpenAI GPT")
    print("   Judge Model : OpenAI GPT-4o")

    components = ["Planning", "Reasoning", "Action", "Reflection"]
    model_choices = ["base", "target"]

    all_combinations = list(itertools.product(model_choices, repeat=len(components)))
    performance_scores: Dict[FrozenSet[str], float] = {}

    for i, combo in enumerate(all_combinations):
        config_map = {
            "planning_llm": agent_models[combo[0]],
            "reasoning_llm": agent_models[combo[1]],
            "action_llm": agent_models[combo[2]],
            "reflection_llm": agent_models[combo[3]],
        }

        coalition = frozenset({components[j] for j, m in enumerate(combo) if m == "target"})
        config_str = f"P:{combo[0]}, R:{combo[1]}, A:{combo[2]}, F:{combo[3]}"
        print(f"\n--- è©•ä¾¡ä¸­ ({i+1}/{len(all_combinations)}): [{config_str}] ---")

        agent = PaperWorkflowAgent(**config_map, tools=tools, verbose=False)

        success_count = 0
        for task in evaluation_tasks:
            try:
                response = agent.run(task["query"])
            except Exception as e:
                response = f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}"
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒé•·ã„å ´åˆã¯çŸ­ç¸®è¡¨ç¤º
            clean_res = response.replace('\n', ' ')[:60]
            print(f"  - Q: {task['query'][:15]}... -> A: {clean_res}...")

            if evaluate_success(response, task):
                success_count += 1
            
            # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼ˆå¿…è¦ã«å¿œã˜ã¦èª¿æ•´ï¼‰
            # time.sleep(1) 

        success_rate = (success_count / len(evaluation_tasks)) * 100.0
        performance_scores[coalition] = success_rate
        print(f"--- çµæœ: æˆåŠŸç‡ = {success_rate:.2f}% ---")

    return performance_scores


# ------------------------------------------------------------------
# 6. ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚¤å€¤è¨ˆç®— (å¤‰æ›´ãªã—)
# ------------------------------------------------------------------
def calculate_shapley_values(
    performance_scores: Dict[FrozenSet[str], float],
    components: List[str],
) -> Dict[str, float]:
    shapley_values = {comp: 0.0 for comp in components}
    n = len(components)

    for component_i in components:
        other_components = [c for c in components if c != component_i]

        for k in range(len(other_components) + 1):
            for S_tuple in itertools.combinations(other_components, k):
                S = frozenset(S_tuple)
                S_with_i = S.union({component_i})

                v_S = performance_scores.get(S, 0.0)
                v_S_with_i = performance_scores.get(S_with_i, 0.0)

                marginal_contribution = v_S_with_i - v_S
                weight = (
                    math.factorial(len(S))
                    * math.factorial(n - len(S) - 1)
                    / math.factorial(n)
                )
                shapley_values[component_i] += weight * marginal_contribution

    return shapley_values


if __name__ == "__main__":
    print(f"baseï¼ˆOllamaï¼‰ãƒ¢ãƒ‡ãƒ«ï¼š{os.environ.get('OLLAMA_MODEL', 'llama3.1')}")
    print(f"targetï¼ˆOpenAIï¼‰ãƒ¢ãƒ‡ãƒ«ï¼š{os.environ.get('OPENAI_MODEL', 'gpt-4o')}")

    scores = run_evaluation()

    print("\n\n--- ğŸ“ˆ å…¨16çµ„ã¿åˆã‚ã›ã®æ€§èƒ½ã‚¹ã‚³ã‚¢ (v(S)) ---")
    sorted_scores = sorted(scores.items(), key=lambda item: len(item[0]))
    for coalition, score in sorted_scores:
        coalition_name = ", ".join(sorted(list(coalition))) if coalition else "âˆ…ï¼ˆå…¨ã¦ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼‰"
        print(f"é€£åˆ [{coalition_name.ljust(45)}]ï¼šæˆåŠŸç‡ {score:.2f}%")

    components_list = ["Planning", "Reasoning", "Action", "Reflection"]
    shapley_results = calculate_shapley_values(scores, components_list)

    print("\n\n--- ğŸ“Š å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚¤å€¤ï¼ˆè²¢çŒ®åº¦ï¼‰ ---")
    print("ã“ã®å€¤ã¯ï¼Œå„éƒ¨å“ã‚’é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«(GPT)ã«æ›¿ãˆãŸéš›ã®å¹³å‡çš„ãªæ€§èƒ½å‘ä¸Šç‡ã‚’ç¤ºã—ã¾ã™ï¼")
    sorted_shapley = sorted(shapley_results.items(), key=lambda item: item[1], reverse=True)
    for component, value in sorted_shapley:
        print(f"è²¢çŒ®åº¦ [{component.ljust(15)}]ï¼š{value:+.2f}")
    print("--------------------------------------------------")